## 中期模型改进计划



### 1. retrieval model (分类model) 


+ 背景知识不够，要更加丰富。
+ 实验下来背景知识比对话历史更重要！
+ 建立一个knowledge数据集，这种retrieval model的数据集长啥样我没调研过，需要相关同学去调研。
+ model应该是一个分类模型，具体怎么做也需要相关同学调研。
+ 最后期望的效果是，我们有一个knowledge database（假设里面包含了100条江姐相关的知识：江姐原名江竹筠；江姐生于1920年；......等等）。每向retrieval model输入一句话，model能够从这100条knowledge中选出top-k个knowledge（选出来的knowledge简称为KGs）。


### 2. instructGPT

+ 有了上面输出的KGs，下面我们需要将KGs和用户的对话一起输入到对话模型（dialogmodel(X)）中，X=<KGs + dialogue> -> dialogmodel(X) -> response. 
+ 在没有retrieval生成的KGs之前，我们利用百度API生成KGs。

#### 2.1 prompt（chat）数据补充（在原来212条样本基础上）

+ 这里我们需要构建两个数据集：
  + 数据集D1：一个是打分数据集，格式为：<X=(responseA，responseB，responseC，responseD，responseE), y=B>C>A>D>E（也就是它们的顺序) >。
  + 数据集D2：二是finetune数据集，格式为 <X=prompt, y=response （注意，这个response需要人工编写）>
  + 数据集具体构建流程：
    + 通过小程序收集同学们的对话历史（下面简称为prompt）。 X=<KGs + prompt> -> model(X) -> responseA，responseB，responseC，responseD，responseE。
      + 如果有合适的response，我们将A，B，C，D，E排序。构成数据集D1.
      + 如果没有合适的response，我们将自己编写response。构成数据集D2.

#### 2.2 reward model（打分模型）

+ 用pair-wise作为损失函数训练reward model，模型由dialogmodel(X)初始化。 
+ 下图是咱们用来标注排序数据的平台。
![](https://i.imgur.com/ikNw9s9.png)


#### 2.3 PPO

+ 暂时放着。




### 3. Pre-training

+ 暂时放着。

